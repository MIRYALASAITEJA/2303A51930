{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNCIOjAr9ijP2Bkr3TrhNKH",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MIRYALASAITEJA/2303A51930/blob/main/IEEE.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ije6EX2aFof8",
        "outputId": "7ff412b3-2c9a-4c68-f920-09e35304bec5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Training Logistic Regression ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Classification Report for Logistic Regression:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       apple       1.00      1.00      1.00        23\n",
            "      banana       1.00      1.00      1.00        21\n",
            "   blackgram       0.85      0.85      0.85        20\n",
            "    chickpea       1.00      1.00      1.00        26\n",
            "     coconut       1.00      1.00      1.00        27\n",
            "      coffee       0.94      1.00      0.97        17\n",
            "      cotton       0.80      0.94      0.86        17\n",
            "      grapes       1.00      1.00      1.00        14\n",
            "        jute       0.87      0.87      0.87        23\n",
            " kidneybeans       1.00      1.00      1.00        20\n",
            "      lentil       0.77      0.91      0.83        11\n",
            "       maize       0.94      0.81      0.87        21\n",
            "       mango       1.00      1.00      1.00        19\n",
            "   mothbeans       0.91      0.88      0.89        24\n",
            "    mungbean       1.00      1.00      1.00        19\n",
            "   muskmelon       1.00      1.00      1.00        17\n",
            "      orange       1.00      1.00      1.00        14\n",
            "      papaya       0.92      0.96      0.94        23\n",
            "  pigeonpeas       1.00      0.91      0.95        23\n",
            " pomegranate       1.00      1.00      1.00        23\n",
            "        rice       0.89      0.84      0.86        19\n",
            "  watermelon       1.00      1.00      1.00        19\n",
            "\n",
            "    accuracy                           0.95       440\n",
            "   macro avg       0.95      0.95      0.95       440\n",
            "weighted avg       0.95      0.95      0.95       440\n",
            "\n",
            "--------------------------------------------------\n",
            "\n",
            "--- Training Support Vector Machine ---\n",
            "\n",
            "Classification Report for Support Vector Machine:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       apple       1.00      1.00      1.00        23\n",
            "      banana       1.00      1.00      1.00        21\n",
            "   blackgram       0.91      1.00      0.95        20\n",
            "    chickpea       1.00      1.00      1.00        26\n",
            "     coconut       1.00      1.00      1.00        27\n",
            "      coffee       1.00      0.94      0.97        17\n",
            "      cotton       0.85      1.00      0.92        17\n",
            "      grapes       1.00      1.00      1.00        14\n",
            "        jute       0.77      1.00      0.87        23\n",
            " kidneybeans       0.95      1.00      0.98        20\n",
            "      lentil       0.79      1.00      0.88        11\n",
            "       maize       1.00      0.86      0.92        21\n",
            "       mango       1.00      1.00      1.00        19\n",
            "   mothbeans       1.00      0.88      0.93        24\n",
            "    mungbean       1.00      1.00      1.00        19\n",
            "   muskmelon       1.00      1.00      1.00        17\n",
            "      orange       1.00      1.00      1.00        14\n",
            "      papaya       1.00      0.96      0.98        23\n",
            "  pigeonpeas       1.00      0.87      0.93        23\n",
            " pomegranate       1.00      1.00      1.00        23\n",
            "        rice       0.93      0.68      0.79        19\n",
            "  watermelon       1.00      1.00      1.00        19\n",
            "\n",
            "    accuracy                           0.96       440\n",
            "   macro avg       0.96      0.96      0.96       440\n",
            "weighted avg       0.97      0.96      0.96       440\n",
            "\n",
            "--------------------------------------------------\n",
            "\n",
            "--- Training Random Forest ---\n",
            "\n",
            "Classification Report for Random Forest:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       apple       1.00      1.00      1.00        23\n",
            "      banana       1.00      1.00      1.00        21\n",
            "   blackgram       1.00      1.00      1.00        20\n",
            "    chickpea       1.00      1.00      1.00        26\n",
            "     coconut       1.00      1.00      1.00        27\n",
            "      coffee       1.00      1.00      1.00        17\n",
            "      cotton       1.00      1.00      1.00        17\n",
            "      grapes       1.00      1.00      1.00        14\n",
            "        jute       0.92      1.00      0.96        23\n",
            " kidneybeans       1.00      1.00      1.00        20\n",
            "      lentil       0.92      1.00      0.96        11\n",
            "       maize       1.00      1.00      1.00        21\n",
            "       mango       1.00      1.00      1.00        19\n",
            "   mothbeans       1.00      0.96      0.98        24\n",
            "    mungbean       1.00      1.00      1.00        19\n",
            "   muskmelon       1.00      1.00      1.00        17\n",
            "      orange       1.00      1.00      1.00        14\n",
            "      papaya       1.00      1.00      1.00        23\n",
            "  pigeonpeas       1.00      1.00      1.00        23\n",
            " pomegranate       1.00      1.00      1.00        23\n",
            "        rice       1.00      0.89      0.94        19\n",
            "  watermelon       1.00      1.00      1.00        19\n",
            "\n",
            "    accuracy                           0.99       440\n",
            "   macro avg       0.99      0.99      0.99       440\n",
            "weighted avg       0.99      0.99      0.99       440\n",
            "\n",
            "--------------------------------------------------\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# Load the dataset\n",
        "df = pd.read_csv('Crop_recommendation.csv')\n",
        "\n",
        "# Prepare the data\n",
        "# Separate features (X) and target (y)\n",
        "X = df[['N', 'P', 'K', 'temperature', 'humidity', 'ph', 'rainfall']]\n",
        "y = df['label']\n",
        "\n",
        "# Encode the categorical target variable 'label' into numerical labels\n",
        "le = LabelEncoder()\n",
        "y_encoded = le.fit_transform(y)\n",
        "\n",
        "# Split the data into training and testing sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y_encoded, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize and train the models\n",
        "models = {\n",
        "    'Logistic Regression': LogisticRegression(max_iter=1000),\n",
        "    'Support Vector Machine': SVC(),\n",
        "    'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "}\n",
        "\n",
        "# Iterate through models, train, predict, and print classification report\n",
        "for name, model in models.items():\n",
        "    print(f\"--- Training {name} ---\")\n",
        "    model.fit(X_train, y_train)\n",
        "    y_pred = model.predict(X_test)\n",
        "    print(f\"\\nClassification Report for {name}:\")\n",
        "    print(classification_report(y_test, y_pred, target_names=le.classes_))\n",
        "    print(\"--------------------------------------------------\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.cluster import KMeans, DBSCAN\n",
        "from sklearn.metrics import silhouette_score\n",
        "\n",
        "# Load the dataset\n",
        "df = pd.read_csv('/content/Crop_recommendation.csv')\n",
        "\n",
        "# Prepare the data for clustering\n",
        "X = df[['N', 'P', 'K', 'temperature', 'humidity', 'ph', 'rainfall']]\n",
        "\n",
        "# Standardize the features\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# --- K-Means Clustering ---\n",
        "n_clusters = df['label'].nunique()  # 22 unique crops\n",
        "kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init='auto')\n",
        "kmeans_labels = kmeans.fit_predict(X_scaled)\n",
        "df['kmeans_cluster'] = kmeans_labels\n",
        "\n",
        "# Silhouette Score for KMeans\n",
        "silhouette_kmeans = silhouette_score(X_scaled, kmeans_labels)\n",
        "\n",
        "print(\"\\n================= K-Means Clustering Results =================\")\n",
        "print(f\"Number of clusters (fixed): {n_clusters}\")\n",
        "print(\"Count of data points in each cluster:\")\n",
        "print(df['kmeans_cluster'].value_counts().sort_index())\n",
        "print(f\"\\nSilhouette Score (KMeans): {silhouette_kmeans:.4f}\")\n",
        "print(\"\\nSample Data with KMeans cluster labels:\")\n",
        "print(df.head(20))\n",
        "\n",
        "# --- DBSCAN Clustering ---\n",
        "dbscan = DBSCAN(eps=0.5, min_samples=5)\n",
        "dbscan_labels = dbscan.fit_predict(X_scaled)\n",
        "df['dbscan_cluster'] = dbscan_labels\n",
        "\n",
        "# Count valid clusters for silhouette (ignoring noise -1)\n",
        "unique_labels = set(dbscan_labels)\n",
        "n_clusters_dbscan = len(unique_labels) - (1 if -1 in unique_labels else 0)\n",
        "\n",
        "# Silhouette Score for DBSCAN (only if more than 1 cluster)\n",
        "if n_clusters_dbscan > 1:\n",
        "    silhouette_dbscan = silhouette_score(X_scaled, dbscan_labels)\n",
        "else:\n",
        "    silhouette_dbscan = None\n",
        "\n",
        "print(\"\\n================= DBSCAN Clustering Results =================\")\n",
        "print(f\"Number of clusters found: {n_clusters_dbscan}\")\n",
        "print(\"Count of data points in each cluster (-1 = outliers):\")\n",
        "print(df['dbscan_cluster'].value_counts().sort_index())\n",
        "\n",
        "if silhouette_dbscan is not None:\n",
        "    print(f\"\\nSilhouette Score (DBSCAN): {silhouette_dbscan:.4f}\")\n",
        "else:\n",
        "    print(\"\\nSilhouette Score (DBSCAN): Not applicable (only 1 cluster found)\")\n",
        "\n",
        "print(\"\\nSample Data with DBSCAN cluster labels:\")\n",
        "print(df.head(20))\n",
        "\n",
        "# --- Save full dataset with clustering results ---\n",
        "df.to_csv(\"Crop_clusters_results.csv\", index=False)\n",
        "print(\"\\nFull results saved as 'Crop_clusters_results.csv'\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v0oBw7kRIi3L",
        "outputId": "2cea1994-89e8-4cab-f012-664673869f59"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "================= K-Means Clustering Results =================\n",
            "Number of clusters (fixed): 22\n",
            "Count of data points in each cluster:\n",
            "kmeans_cluster\n",
            "0     143\n",
            "1     103\n",
            "2     130\n",
            "3     157\n",
            "4     200\n",
            "5      48\n",
            "6      99\n",
            "7      53\n",
            "8     157\n",
            "9     119\n",
            "10    100\n",
            "11    102\n",
            "12    110\n",
            "13     42\n",
            "14     52\n",
            "15    103\n",
            "16    151\n",
            "17     33\n",
            "18    100\n",
            "19    102\n",
            "20     59\n",
            "21     37\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Silhouette Score (KMeans): 0.3446\n",
            "\n",
            "Sample Data with KMeans cluster labels:\n",
            "     N   P   K  temperature   humidity        ph    rainfall label  \\\n",
            "0   90  42  43    20.879744  82.002744  6.502985  202.935536  rice   \n",
            "1   85  58  41    21.770462  80.319644  7.038096  226.655537  rice   \n",
            "2   60  55  44    23.004459  82.320763  7.840207  263.964248  rice   \n",
            "3   74  35  40    26.491096  80.158363  6.980401  242.864034  rice   \n",
            "4   78  42  42    20.130175  81.604873  7.628473  262.717340  rice   \n",
            "5   69  37  42    23.058049  83.370118  7.073454  251.055000  rice   \n",
            "6   69  55  38    22.708838  82.639414  5.700806  271.324860  rice   \n",
            "7   94  53  40    20.277744  82.894086  5.718627  241.974195  rice   \n",
            "8   89  54  38    24.515881  83.535216  6.685346  230.446236  rice   \n",
            "9   68  58  38    23.223974  83.033227  6.336254  221.209196  rice   \n",
            "10  91  53  40    26.527235  81.417538  5.386168  264.614870  rice   \n",
            "11  90  46  42    23.978982  81.450616  7.502834  250.083234  rice   \n",
            "12  78  58  44    26.800796  80.886848  5.108682  284.436457  rice   \n",
            "13  93  56  36    24.014976  82.056872  6.984354  185.277339  rice   \n",
            "14  94  50  37    25.665852  80.663850  6.948020  209.586971  rice   \n",
            "15  60  48  39    24.282094  80.300256  7.042299  231.086335  rice   \n",
            "16  85  38  41    21.587118  82.788371  6.249051  276.655246  rice   \n",
            "17  91  35  39    23.793920  80.418180  6.970860  206.261186  rice   \n",
            "18  77  38  36    21.865252  80.192301  5.953933  224.555017  rice   \n",
            "19  88  35  40    23.579436  83.587603  5.853932  291.298662  rice   \n",
            "\n",
            "    kmeans_cluster  \n",
            "0                3  \n",
            "1                3  \n",
            "2                3  \n",
            "3                3  \n",
            "4                3  \n",
            "5                3  \n",
            "6               20  \n",
            "7               20  \n",
            "8                3  \n",
            "9               20  \n",
            "10              20  \n",
            "11               3  \n",
            "12              20  \n",
            "13               3  \n",
            "14               3  \n",
            "15               3  \n",
            "16              20  \n",
            "17               3  \n",
            "18              20  \n",
            "19              20  \n",
            "\n",
            "================= DBSCAN Clustering Results =================\n",
            "Number of clusters found: 45\n",
            "Count of data points in each cluster (-1 = outliers):\n",
            "dbscan_cluster\n",
            "-1     978\n",
            " 0      62\n",
            " 1      10\n",
            " 2       8\n",
            " 3      56\n",
            " 4      29\n",
            " 5       4\n",
            " 6      69\n",
            " 7     100\n",
            " 8      82\n",
            " 9       5\n",
            " 10     14\n",
            " 11      9\n",
            " 12     15\n",
            " 13      5\n",
            " 14      4\n",
            " 15     77\n",
            " 16      5\n",
            " 17     10\n",
            " 18     96\n",
            " 19      7\n",
            " 20     10\n",
            " 21      6\n",
            " 22     10\n",
            " 23      5\n",
            " 24      5\n",
            " 25      4\n",
            " 26      9\n",
            " 27      5\n",
            " 28      5\n",
            " 29      8\n",
            " 30      5\n",
            " 31    200\n",
            " 32    100\n",
            " 33      5\n",
            " 34      6\n",
            " 35      5\n",
            " 36     27\n",
            " 37     14\n",
            " 38     10\n",
            " 39      4\n",
            " 40     34\n",
            " 41     51\n",
            " 42     12\n",
            " 43     10\n",
            " 44      5\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Silhouette Score (DBSCAN): -0.0936\n",
            "\n",
            "Sample Data with DBSCAN cluster labels:\n",
            "     N   P   K  temperature   humidity        ph    rainfall label  \\\n",
            "0   90  42  43    20.879744  82.002744  6.502985  202.935536  rice   \n",
            "1   85  58  41    21.770462  80.319644  7.038096  226.655537  rice   \n",
            "2   60  55  44    23.004459  82.320763  7.840207  263.964248  rice   \n",
            "3   74  35  40    26.491096  80.158363  6.980401  242.864034  rice   \n",
            "4   78  42  42    20.130175  81.604873  7.628473  262.717340  rice   \n",
            "5   69  37  42    23.058049  83.370118  7.073454  251.055000  rice   \n",
            "6   69  55  38    22.708838  82.639414  5.700806  271.324860  rice   \n",
            "7   94  53  40    20.277744  82.894086  5.718627  241.974195  rice   \n",
            "8   89  54  38    24.515881  83.535216  6.685346  230.446236  rice   \n",
            "9   68  58  38    23.223974  83.033227  6.336254  221.209196  rice   \n",
            "10  91  53  40    26.527235  81.417538  5.386168  264.614870  rice   \n",
            "11  90  46  42    23.978982  81.450616  7.502834  250.083234  rice   \n",
            "12  78  58  44    26.800796  80.886848  5.108682  284.436457  rice   \n",
            "13  93  56  36    24.014976  82.056872  6.984354  185.277339  rice   \n",
            "14  94  50  37    25.665852  80.663850  6.948020  209.586971  rice   \n",
            "15  60  48  39    24.282094  80.300256  7.042299  231.086335  rice   \n",
            "16  85  38  41    21.587118  82.788371  6.249051  276.655246  rice   \n",
            "17  91  35  39    23.793920  80.418180  6.970860  206.261186  rice   \n",
            "18  77  38  36    21.865252  80.192301  5.953933  224.555017  rice   \n",
            "19  88  35  40    23.579436  83.587603  5.853932  291.298662  rice   \n",
            "\n",
            "    kmeans_cluster  dbscan_cluster  \n",
            "0                3               0  \n",
            "1                3              -1  \n",
            "2                3              -1  \n",
            "3                3              -1  \n",
            "4                3              -1  \n",
            "5                3              -1  \n",
            "6               20              -1  \n",
            "7               20              -1  \n",
            "8                3              -1  \n",
            "9               20              -1  \n",
            "10              20              -1  \n",
            "11               3              -1  \n",
            "12              20              -1  \n",
            "13               3               0  \n",
            "14               3              -1  \n",
            "15               3              -1  \n",
            "16              20              -1  \n",
            "17               3               0  \n",
            "18              20              -1  \n",
            "19              20              -1  \n",
            "\n",
            "Full results saved as 'Crop_clusters_results.csv'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# Load the dataset\n",
        "df = pd.read_csv('Crop_recommendation.csv')\n",
        "\n",
        "# Prepare the data\n",
        "# Separate features (X) and target (y)\n",
        "X = df[['N', 'P', 'K', 'temperature', 'humidity', 'ph', 'rainfall']]\n",
        "y = df['label']\n",
        "\n",
        "# Encode the categorical target variable 'label' into numerical labels\n",
        "le = LabelEncoder()\n",
        "y_encoded = le.fit_transform(y)\n",
        "\n",
        "# One-hot encode the numerical labels for deep learning\n",
        "y_one_hot = to_categorical(y_encoded)\n",
        "\n",
        "# Scale the features for better model performance\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y_one_hot, test_size=0.2, random_state=42)\n",
        "\n",
        "# Get the number of features and output classes\n",
        "n_features = X_train.shape[1]\n",
        "n_classes = y_one_hot.shape[1]\n",
        "\n",
        "# Build the Sequential deep learning model\n",
        "model = Sequential()\n",
        "model.add(Dense(128, input_shape=(n_features,), activation='relu'))\n",
        "model.add(Dense(64, activation='relu'))\n",
        "model.add(Dense(n_classes, activation='softmax'))\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Print a summary of the model architecture\n",
        "print(model.summary())\n",
        "\n",
        "# Train the model\n",
        "print(\"--- Training the Deep Learning Model ---\")\n",
        "model.fit(X_train, y_train, epochs=50, batch_size=32, verbose=1, validation_split=0.1)\n",
        "\n",
        "# Evaluate the model on the test data\n",
        "loss, accuracy = model.evaluate(X_test, y_test, verbose=0)\n",
        "print(f\"\\nModel Accuracy on Test Data: {accuracy:.4f}\")\n",
        "\n",
        "# Generate predictions and classification report\n",
        "y_pred_one_hot = model.predict(X_test)\n",
        "y_pred = np.argmax(y_pred_one_hot, axis=1)\n",
        "y_test_labels = np.argmax(y_test, axis=1)\n",
        "\n",
        "print(\"\\n--- Classification Report ---\")\n",
        "print(classification_report(y_test_labels, y_pred, target_names=le.classes_))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "NJJPBdaqInQd",
        "outputId": "7b59485d-4c7d-4152-9769-3f55274f8292"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/core/dense.py:93: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │         \u001b[38;5;34m1,024\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │         \u001b[38;5;34m8,256\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m22\u001b[0m)             │         \u001b[38;5;34m1,430\u001b[0m │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,024</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">8,256</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">22</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,430</span> │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m10,710\u001b[0m (41.84 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">10,710</span> (41.84 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m10,710\u001b[0m (41.84 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">10,710</span> (41.84 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "None\n",
            "--- Training the Deep Learning Model ---\n",
            "Epoch 1/50\n",
            "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - accuracy: 0.2728 - loss: 2.8228 - val_accuracy: 0.6307 - val_loss: 1.9453\n",
            "Epoch 2/50\n",
            "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7020 - loss: 1.6785 - val_accuracy: 0.8466 - val_loss: 0.9171\n",
            "Epoch 3/50\n",
            "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8822 - loss: 0.7684 - val_accuracy: 0.8864 - val_loss: 0.5204\n",
            "Epoch 4/50\n",
            "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9126 - loss: 0.4403 - val_accuracy: 0.8807 - val_loss: 0.3896\n",
            "Epoch 5/50\n",
            "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9347 - loss: 0.2998 - val_accuracy: 0.9318 - val_loss: 0.2882\n",
            "Epoch 6/50\n",
            "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9421 - loss: 0.2379 - val_accuracy: 0.9205 - val_loss: 0.2410\n",
            "Epoch 7/50\n",
            "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9586 - loss: 0.1806 - val_accuracy: 0.9545 - val_loss: 0.1937\n",
            "Epoch 8/50\n",
            "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9674 - loss: 0.1543 - val_accuracy: 0.9489 - val_loss: 0.1806\n",
            "Epoch 9/50\n",
            "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9593 - loss: 0.1375 - val_accuracy: 0.9602 - val_loss: 0.1539\n",
            "Epoch 10/50\n",
            "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9705 - loss: 0.1110 - val_accuracy: 0.9659 - val_loss: 0.1332\n",
            "Epoch 11/50\n",
            "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9785 - loss: 0.0967 - val_accuracy: 0.9716 - val_loss: 0.1211\n",
            "Epoch 12/50\n",
            "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9800 - loss: 0.0869 - val_accuracy: 0.9489 - val_loss: 0.1236\n",
            "Epoch 13/50\n",
            "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9716 - loss: 0.0849 - val_accuracy: 0.9773 - val_loss: 0.0964\n",
            "Epoch 14/50\n",
            "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9818 - loss: 0.0815 - val_accuracy: 0.9773 - val_loss: 0.0901\n",
            "Epoch 15/50\n",
            "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9888 - loss: 0.0645 - val_accuracy: 0.9773 - val_loss: 0.0833\n",
            "Epoch 16/50\n",
            "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9848 - loss: 0.0633 - val_accuracy: 0.9602 - val_loss: 0.0992\n",
            "Epoch 17/50\n",
            "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9816 - loss: 0.0634 - val_accuracy: 0.9773 - val_loss: 0.0657\n",
            "Epoch 18/50\n",
            "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9841 - loss: 0.0524 - val_accuracy: 0.9659 - val_loss: 0.0842\n",
            "Epoch 19/50\n",
            "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9848 - loss: 0.0538 - val_accuracy: 0.9773 - val_loss: 0.0551\n",
            "Epoch 20/50\n",
            "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9882 - loss: 0.0452 - val_accuracy: 0.9773 - val_loss: 0.0612\n",
            "Epoch 21/50\n",
            "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9864 - loss: 0.0468 - val_accuracy: 0.9830 - val_loss: 0.0602\n",
            "Epoch 22/50\n",
            "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9878 - loss: 0.0461 - val_accuracy: 0.9886 - val_loss: 0.0483\n",
            "Epoch 23/50\n",
            "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9857 - loss: 0.0485 - val_accuracy: 0.9943 - val_loss: 0.0467\n",
            "Epoch 24/50\n",
            "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9877 - loss: 0.0471 - val_accuracy: 0.9830 - val_loss: 0.0603\n",
            "Epoch 25/50\n",
            "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9913 - loss: 0.0340 - val_accuracy: 0.9602 - val_loss: 0.0833\n",
            "Epoch 26/50\n",
            "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9878 - loss: 0.0417 - val_accuracy: 0.9830 - val_loss: 0.0584\n",
            "Epoch 27/50\n",
            "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9957 - loss: 0.0305 - val_accuracy: 0.9773 - val_loss: 0.0520\n",
            "Epoch 28/50\n",
            "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9894 - loss: 0.0381 - val_accuracy: 0.9886 - val_loss: 0.0565\n",
            "Epoch 29/50\n",
            "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9942 - loss: 0.0338 - val_accuracy: 0.9886 - val_loss: 0.0386\n",
            "Epoch 30/50\n",
            "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9929 - loss: 0.0293 - val_accuracy: 0.9830 - val_loss: 0.0434\n",
            "Epoch 31/50\n",
            "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9947 - loss: 0.0285 - val_accuracy: 0.9830 - val_loss: 0.0424\n",
            "Epoch 32/50\n",
            "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9894 - loss: 0.0318 - val_accuracy: 0.9886 - val_loss: 0.0473\n",
            "Epoch 33/50\n",
            "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9940 - loss: 0.0307 - val_accuracy: 0.9943 - val_loss: 0.0305\n",
            "Epoch 34/50\n",
            "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9933 - loss: 0.0258 - val_accuracy: 0.9716 - val_loss: 0.0504\n",
            "Epoch 35/50\n",
            "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9876 - loss: 0.0304 - val_accuracy: 1.0000 - val_loss: 0.0300\n",
            "Epoch 36/50\n",
            "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9958 - loss: 0.0271 - val_accuracy: 0.9659 - val_loss: 0.0582\n",
            "Epoch 37/50\n",
            "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9899 - loss: 0.0307 - val_accuracy: 0.9886 - val_loss: 0.0303\n",
            "Epoch 38/50\n",
            "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9938 - loss: 0.0226 - val_accuracy: 0.9943 - val_loss: 0.0307\n",
            "Epoch 39/50\n",
            "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9957 - loss: 0.0229 - val_accuracy: 0.9830 - val_loss: 0.0342\n",
            "Epoch 40/50\n",
            "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.9923 - loss: 0.0239 - val_accuracy: 0.9830 - val_loss: 0.0360\n",
            "Epoch 41/50\n",
            "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.9943 - loss: 0.0267 - val_accuracy: 0.9886 - val_loss: 0.0236\n",
            "Epoch 42/50\n",
            "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9956 - loss: 0.0201 - val_accuracy: 0.9886 - val_loss: 0.0293\n",
            "Epoch 43/50\n",
            "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9946 - loss: 0.0220 - val_accuracy: 0.9773 - val_loss: 0.0405\n",
            "Epoch 44/50\n",
            "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9933 - loss: 0.0197 - val_accuracy: 0.9830 - val_loss: 0.0385\n",
            "Epoch 45/50\n",
            "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9961 - loss: 0.0215 - val_accuracy: 0.9830 - val_loss: 0.0293\n",
            "Epoch 46/50\n",
            "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9919 - loss: 0.0223 - val_accuracy: 0.9943 - val_loss: 0.0302\n",
            "Epoch 47/50\n",
            "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9965 - loss: 0.0172 - val_accuracy: 0.9886 - val_loss: 0.0227\n",
            "Epoch 48/50\n",
            "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9947 - loss: 0.0200 - val_accuracy: 0.9943 - val_loss: 0.0332\n",
            "Epoch 49/50\n",
            "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9946 - loss: 0.0180 - val_accuracy: 0.9943 - val_loss: 0.0313\n",
            "Epoch 50/50\n",
            "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9956 - loss: 0.0147 - val_accuracy: 0.9830 - val_loss: 0.0289\n",
            "\n",
            "Model Accuracy on Test Data: 0.9705\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step\n",
            "\n",
            "--- Classification Report ---\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       apple       1.00      1.00      1.00        23\n",
            "      banana       1.00      1.00      1.00        21\n",
            "   blackgram       1.00      0.95      0.97        20\n",
            "    chickpea       1.00      1.00      1.00        26\n",
            "     coconut       1.00      1.00      1.00        27\n",
            "      coffee       1.00      0.94      0.97        17\n",
            "      cotton       1.00      1.00      1.00        17\n",
            "      grapes       1.00      1.00      1.00        14\n",
            "        jute       0.86      0.83      0.84        23\n",
            " kidneybeans       0.95      0.95      0.95        20\n",
            "      lentil       0.73      1.00      0.85        11\n",
            "       maize       1.00      1.00      1.00        21\n",
            "       mango       1.00      1.00      1.00        19\n",
            "   mothbeans       1.00      0.88      0.93        24\n",
            "    mungbean       1.00      1.00      1.00        19\n",
            "   muskmelon       1.00      1.00      1.00        17\n",
            "      orange       1.00      1.00      1.00        14\n",
            "      papaya       1.00      1.00      1.00        23\n",
            "  pigeonpeas       0.96      0.96      0.96        23\n",
            " pomegranate       1.00      1.00      1.00        23\n",
            "        rice       0.81      0.89      0.85        19\n",
            "  watermelon       1.00      1.00      1.00        19\n",
            "\n",
            "    accuracy                           0.97       440\n",
            "   macro avg       0.97      0.97      0.97       440\n",
            "weighted avg       0.97      0.97      0.97       440\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
        "from sklearn.ensemble import VotingClassifier, StackingClassifier\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# Load the dataset\n",
        "df = pd.read_csv('Crop_recommendation.csv')\n",
        "\n",
        "# Prepare the data\n",
        "# Separate features (X) and target (y)\n",
        "X = df[['N', 'P', 'K', 'temperature', 'humidity', 'ph', 'rainfall']]\n",
        "y = df['label']\n",
        "\n",
        "# Encode the categorical target variable 'label' into numerical labels\n",
        "le = LabelEncoder()\n",
        "y_encoded = le.fit_transform(y)\n",
        "\n",
        "# Scale the features for better model performance\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y_encoded, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define the base estimators (individual models) for the ensembles\n",
        "estimators = [\n",
        "    ('log_reg', LogisticRegression(max_iter=1000, random_state=42)),\n",
        "    ('rf_clf', RandomForestClassifier(n_estimators=100, random_state=42)),\n",
        "    ('gb_clf', GradientBoostingClassifier(n_estimators=100, random_state=42))\n",
        "]\n",
        "\n",
        "## Voting Classifier (Hard Voting)\n",
        "# The VotingClassifier aggregates predictions from multiple models\n",
        "# and uses the majority vote to determine the final prediction.\n",
        "print(\"--- Training VotingClassifier ---\")\n",
        "eclf1 = VotingClassifier(estimators=estimators, voting='hard')\n",
        "eclf1 = eclf1.fit(X_train, y_train)\n",
        "y_pred_voting = eclf1.predict(X_test)\n",
        "\n",
        "print(\"\\nClassification Report for VotingClassifier:\")\n",
        "print(classification_report(y_test, y_pred_voting, target_names=le.classes_))\n",
        "print(\"\\n--------------------------------------------------\\n\")\n",
        "\n",
        "## Stacking Classifier\n",
        "# Stacking uses the predictions of the base models as features\n",
        "# for a final, meta-model (here, a Logistic Regression model).\n",
        "print(\"--- Training StackingClassifier ---\")\n",
        "stacking_clf = StackingClassifier(\n",
        "    estimators=estimators,\n",
        "    final_estimator=LogisticRegression(random_state=42)\n",
        ")\n",
        "stacking_clf.fit(X_train, y_train)\n",
        "y_pred_stacking = stacking_clf.predict(X_test)\n",
        "\n",
        "print(\"\\nClassification Report for StackingClassifier:\")\n",
        "print(classification_report(y_test, y_pred_stacking, target_names=le.classes_))\n",
        "print(\"\\n--------------------------------------------------\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d-8TBxYzItm1",
        "outputId": "d85e1007-4e75-4c1a-cc1d-efda6a98e806"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Training VotingClassifier ---\n",
            "\n",
            "Classification Report for VotingClassifier:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       apple       1.00      1.00      1.00        23\n",
            "      banana       1.00      1.00      1.00        21\n",
            "   blackgram       0.95      1.00      0.98        20\n",
            "    chickpea       1.00      1.00      1.00        26\n",
            "     coconut       1.00      1.00      1.00        27\n",
            "      coffee       1.00      1.00      1.00        17\n",
            "      cotton       1.00      1.00      1.00        17\n",
            "      grapes       1.00      1.00      1.00        14\n",
            "        jute       0.88      1.00      0.94        23\n",
            " kidneybeans       1.00      1.00      1.00        20\n",
            "      lentil       0.92      1.00      0.96        11\n",
            "       maize       1.00      1.00      1.00        21\n",
            "       mango       1.00      1.00      1.00        19\n",
            "   mothbeans       1.00      0.96      0.98        24\n",
            "    mungbean       1.00      1.00      1.00        19\n",
            "   muskmelon       1.00      1.00      1.00        17\n",
            "      orange       1.00      1.00      1.00        14\n",
            "      papaya       1.00      1.00      1.00        23\n",
            "  pigeonpeas       1.00      0.96      0.98        23\n",
            " pomegranate       1.00      1.00      1.00        23\n",
            "        rice       1.00      0.84      0.91        19\n",
            "  watermelon       1.00      1.00      1.00        19\n",
            "\n",
            "    accuracy                           0.99       440\n",
            "   macro avg       0.99      0.99      0.99       440\n",
            "weighted avg       0.99      0.99      0.99       440\n",
            "\n",
            "\n",
            "--------------------------------------------------\n",
            "\n",
            "--- Training StackingClassifier ---\n",
            "\n",
            "Classification Report for StackingClassifier:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       apple       1.00      1.00      1.00        23\n",
            "      banana       1.00      1.00      1.00        21\n",
            "   blackgram       1.00      1.00      1.00        20\n",
            "    chickpea       1.00      1.00      1.00        26\n",
            "     coconut       1.00      1.00      1.00        27\n",
            "      coffee       1.00      1.00      1.00        17\n",
            "      cotton       0.94      1.00      0.97        17\n",
            "      grapes       1.00      1.00      1.00        14\n",
            "        jute       0.92      1.00      0.96        23\n",
            " kidneybeans       1.00      1.00      1.00        20\n",
            "      lentil       0.92      1.00      0.96        11\n",
            "       maize       1.00      0.95      0.98        21\n",
            "       mango       1.00      1.00      1.00        19\n",
            "   mothbeans       1.00      0.96      0.98        24\n",
            "    mungbean       1.00      1.00      1.00        19\n",
            "   muskmelon       1.00      1.00      1.00        17\n",
            "      orange       1.00      1.00      1.00        14\n",
            "      papaya       1.00      1.00      1.00        23\n",
            "  pigeonpeas       1.00      1.00      1.00        23\n",
            " pomegranate       1.00      1.00      1.00        23\n",
            "        rice       1.00      0.89      0.94        19\n",
            "  watermelon       1.00      1.00      1.00        19\n",
            "\n",
            "    accuracy                           0.99       440\n",
            "   macro avg       0.99      0.99      0.99       440\n",
            "weighted avg       0.99      0.99      0.99       440\n",
            "\n",
            "\n",
            "--------------------------------------------------\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# Load the dataset\n",
        "df = pd.read_csv('Crop_recommendation.csv')\n",
        "\n",
        "# Prepare the data\n",
        "# Separate features (X) and target (y)\n",
        "X = df[['N', 'P', 'K', 'temperature', 'humidity', 'ph', 'rainfall']]\n",
        "y = df['label']\n",
        "\n",
        "# Encode the categorical target variable 'label' into numerical labels\n",
        "le = LabelEncoder()\n",
        "y_encoded = le.fit_transform(y)\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y_encoded, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize and train the Gradient Boosting Classifier\n",
        "# This algorithm builds a series of decision trees and combines their predictions.\n",
        "gb_model = GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=42)\n",
        "\n",
        "print(\"--- Training the Gradient Boosting Model ---\")\n",
        "gb_model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred_gb = gb_model.predict(X_test)\n",
        "\n",
        "# Print the classification report\n",
        "print(\"\\n--- Classification Report for Gradient Boosting Model ---\")\n",
        "print(classification_report(y_test, y_pred_gb, target_names=le.classes_))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ck7pRMLaZlYP",
        "outputId": "a9b15ac6-4937-4f84-8c1b-f927f4291e06"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Training the Gradient Boosting Model ---\n",
            "\n",
            "--- Classification Report for Gradient Boosting Model ---\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       apple       1.00      1.00      1.00        23\n",
            "      banana       1.00      1.00      1.00        21\n",
            "   blackgram       0.95      1.00      0.98        20\n",
            "    chickpea       1.00      1.00      1.00        26\n",
            "     coconut       1.00      0.96      0.98        27\n",
            "      coffee       1.00      1.00      1.00        17\n",
            "      cotton       0.94      1.00      0.97        17\n",
            "      grapes       1.00      1.00      1.00        14\n",
            "        jute       0.82      1.00      0.90        23\n",
            " kidneybeans       1.00      1.00      1.00        20\n",
            "      lentil       0.92      1.00      0.96        11\n",
            "       maize       1.00      0.95      0.98        21\n",
            "       mango       1.00      1.00      1.00        19\n",
            "   mothbeans       1.00      0.96      0.98        24\n",
            "    mungbean       1.00      1.00      1.00        19\n",
            "   muskmelon       1.00      1.00      1.00        17\n",
            "      orange       1.00      1.00      1.00        14\n",
            "      papaya       1.00      1.00      1.00        23\n",
            "  pigeonpeas       1.00      0.96      0.98        23\n",
            " pomegranate       1.00      1.00      1.00        23\n",
            "        rice       1.00      0.79      0.88        19\n",
            "  watermelon       1.00      1.00      1.00        19\n",
            "\n",
            "    accuracy                           0.98       440\n",
            "   macro avg       0.98      0.98      0.98       440\n",
            "weighted avg       0.98      0.98      0.98       440\n",
            "\n"
          ]
        }
      ]
    }
  ]
}